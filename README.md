# Neural-Guided Genetic Programming for Symbolic Regression

A Unity bsed implementation of advanced evolutionary algorithms for discovering mathematical expressions from data, enhanced with neural network guidance and multiobjective optimization
## Mathematical Foundation

### Expression Representation

Expressions are represented as binary trees where:

- **Internal nodes**: Mathematical operators ‚àà {+, ‚àí, √ó, √∑, sin, cos, log, exp, ^, ‚àö}
- **Leaf nodes**: Variables (x) or constants (c ‚àà ‚Ñù)

Example: f(x) = x¬≤ + 1 ‚Üí Tree(+, Tree(^, x, 2), 1)

### Fitness Function

The fitness œÜ of an individual is computed as:
```js
œÜ = -(MSE + Œª¬∑C)
```


where:
- MSE = (1/n)Œ£·µ¢‚Çå‚ÇÅ‚Åø (y·µ¢ - ≈∑·µ¢)¬≤ is the mean squared error
- C = number of nodes in expression tree (complexity measure)
- Œª ‚àà ‚Ñù‚Å∫ is the complexity penalty weight
- n is the number of data points

### Multi-Objective Optimization

The system implements NSGA-II (Non-dominated Sorting Genetic Algorithm II) to optimize two competing objectives:

| Objective | Goal | Formula |
|-----------|------|---------|
| Accuracy | Minimize prediction error | min MSE(f) |
| Simplicity | Minimize expression complexity | min \|nodes(f)\| |

Pareto dominance: f‚ÇÅ ‚âª f‚ÇÇ iff MSE(f‚ÇÅ) ‚â§ MSE(f‚ÇÇ) ‚àß C(f‚ÇÅ) ‚â§ C(f‚ÇÇ) ‚àß (MSE(f‚ÇÅ) < MSE(f‚ÇÇ) ‚à® C(f‚ÇÅ) < C(f‚ÇÇ))

## Algorithm Components

### Evolutionary Operators

| Operator | Type | Probability | Description |
|----------|------|-------------|-------------|
| Constant Mutation | Point | 0.25 | Œ¥c ~ ùí∞(-œÉ, œÉ), c' = c + Œ¥c |
| Subtree Mutation | Structural | 0.30 | Replace random subtree with new random tree |
| Node Deletion | Structural | 0.20 | Remove node, promote child |
| Simplification | Algebraic | 0.25 | Collapse constant-only subtrees |
| Crossover | Recombination | 0.70 | Exchange random subtrees between parents |

### Island Model Architecture

The system employs a parallel island model for population diversity:
- Islands: {I‚ÇÅ, I‚ÇÇ, I‚ÇÉ, I‚ÇÑ}
- Topology: Ring (I·µ¢ ‚Üí I‚Çç·µ¢‚Çä‚ÇÅ‚Çé mod 4)
- Migration interval: œÑ = 20 generations
- Migration rate: m = 3 individuals per island
  
Each island maintains different evolutionary parameters:

| Island | Population | Mutation Rate Œº | Complexity Weight Œª | Max Depth d |
|--------|-----------|----------------|---------------------|-------------|
| I‚ÇÅ | 250 | 0.70 | 1.0 | 4 |
| I‚ÇÇ | 250 | 0.75 | 1.5 | 5 |
| I‚ÇÉ | 250 | 0.80 | 2.0 | 4 |
| I‚ÇÑ | 250 | 0.85 | 2.5 | 5 |

### Neural Guidance System

A shallow neural network predicts expression fitness before evaluation:

Architecture: ‚Üí ‚Üí‚Äã‚Äã
Activation: ReLU(x) = max(0, x)
Training: Online gradient descent with Œ∑

```js
P(accept worse solution) = exp(ŒîœÜ / T)
T(t) = T‚ÇÄ ¬∑ Œ±·µó
```


where T‚ÇÄ = 1.0, Œ± = 0.995 (cooling rate)

## Performance Characteristics

### Computational Complexity

| Operation | Time Complexity | Space Complexity |
|-----------|----------------|------------------|
| Tree evaluation | O(n¬∑m) | O(d) |
| Population evolution | O(p¬∑n¬∑m) | O(p¬∑d) |
| NSGA-II sorting | O(p¬≤¬∑k) | O(p) |
| Neural training | O(e¬∑s¬∑h¬≤) | O(h¬≤) |

where:
- n = number of data points
- m = average expression size
- d = tree depth
- p = population size
- k = number of objectives (2)
- e = training epochs
- s = training set size
- h = hidden layer size

### Benchmark Results

| Function | Expression | Generations | Final MSE | Time (s) |
|----------|------------|-------------|-----------|----------|
| Linear | 2x + 1 | 15 | 1.2√ó10‚Åª‚Å∏ | 0.8 |
| Quadratic | x¬≤ + 1 | 28 | 3.4√ó10‚Åª‚Å∑ | 1.5 |
| Trigonometric | sin(x) | 142 | 2.1√ó10‚Åª‚Å¥ | 7.3 |
| Rational | (x+1)/(x+2) | 89 | 1.8√ó10‚Åª‚Åµ | 4.2 |

## Installation

### Prerequisites

- Unity 2021.3 LTS or later
- .NET Framework 4.7.1+
- TextMeshPro package

### Setup

1. Clone the repository:
git clone https://github.com/InboraStudio/Neural-Guided-Genetic-Programming-for-Symbolic-Regression.git
cd Neural-Guided-Genetic-Programming-for-Symbolic-Regression

2. Open project in Unity

3. Install TextMeshPro:
   - Window ‚Üí Package Manager ‚Üí Search "TextMesh Pro" ‚Üí Install

4. Import all scripts into Assets/Scripts/

## Usage

### Basic Configuration

// Create test dataset
float[] inputs = new float[] { 0, 1, 2, 3, 4, 5 };
float[] outputs = new float[] { 1, 2, 5, 10, 17, 26 }; // x¬≤ + 1

// Configure GP parameters
numberOfIslands = 4;
populationPerIsland = 250;
maxGenerations = 1000;
deathRate = 0.7f;
complexityWeight = 2.0f;

### Running Evolution

1. Attach `AdvancedSymbolicRegressionGP` to GameObject
2. Configure parameters in Inspector
3. Press Play
4. Monitor Console for evolution progress

### Visualization

Attach `RealTimeGraphSystem` for live plotting:
- Top-left: Evolution metrics (fitness, MSE, complexity)
- Top-right: Function fit comparison
- Bottom: Current best expression and statistics

## Repository Structure


## Algorithm Configuration Parameters

### Recommended Settings

| Parameter | Small Dataset (<50 pts) | Medium (50-500) | Large (>500) |
|-----------|------------------------|-----------------|--------------|
| Population/Island | 100-250 | 250-500 | 500-1000 |
| Max Generations | 100-500 | 500-1000 | 1000-5000 |
| Complexity Weight Œª | 1.0-2.0 | 2.0-5.0 | 0.5-1.0 |
| Migration Interval | 10-20 | 20-50 | 50-100 |

### Parameter Sensitivity

High impact parameters (tune first):
- Population size: Linear relationship with solution quality
- Complexity weight: Balances accuracy vs. simplicity
- Death rate: Controls selection pressure

Low impact parameters (use defaults):
- Migration topology: Ring vs. Star (< 5% difference)
- Neural guidance interval: 30-100 generations
- Cooling rate: 0.99-0.999

## Theoretical Background

### References

This implementation is based on the following research:

1. **Genetic Programming**: Koza, J. R. (1992). *Genetic Programming: On the Programming of Computers by Means of Natural Selection*. MIT Press.

2. **Island Models**: Whitley, D., Rana, S., & Heckendorn, R. B. (1998). *The Island Model Genetic Algorithm: On Separability, Population Size and Convergence*. Journal of Computing and Information Technology.

3. **NSGA-II**: Deb, K., et al. (2002). *A Fast and Elitist Multiobjective Genetic Algorithm: NSGA-II*. IEEE Transactions on Evolutionary Computation, 6(2), 182-197.

4. **Symbolic Regression**: Schmidt, M., & Lipson, H. (2009). *Distilling Free-Form Natural Laws from Experimental Data*. Science, 324(5923), 81-85.

5. **Neural-Guided Evolution**: Lample, G., & Charton, F. (2019). *Deep Learning for Symbolic Mathematics*. arXiv:1912.01412.

## Applications

- **Physics**: Discovery of governing equations from experimental data
- **Engineering**: System identification and model fitting
- **Economics**: Empirical relationship modeling
- **Biology**: Gene regulatory network inference
- **Data Science**: Feature engineering and transformation discovery

## Performance Optimization

### Parallel Execution

Island model enables parallel evolution:
```cs
Parallel.ForEach(islands, island => {
island.EvolveGeneration(inputData, outputData, deathRate, eliteCount);
});
```

Expected speedup: S ‚âà N_islands / (1 + communication_overhead)

### Memory Usage

Approximate memory footprint:
```cs
Memory ‚âà (p √ó d √ó 4 bytes) √ó N_islands + (h¬≤ √ó 4 bytes)
‚âà (1000 √ó 20 √ó 4) √ó 4 + (64¬≤ √ó 4)
‚âà 336 KB
```

## Known Limitations

1. **Discontinuous Functions**: Struggles with step functions and piecewise definitions
2. **High-Frequency Oscillations**: May underfit rapidly oscillating functions
3. **Dimensional Analysis**: Does not enforce physical unit consistency
4. **Computational Scaling**: O(n¬∑m) evaluation cost limits large datasets

## Future Enhancements

- [ ] Multi-variable support (f: ‚Ñù‚Åø ‚Üí ‚Ñù)
- [ ] Dimensional analysis constraints
- [ ] GPU-accelerated tree evaluation
- [ ] Automatic operator selection based on domain
- [ ] Integration with Unity ML-Agents

## Contributing

Contributions are welcome. Please follow these guidelines:

1. Fork the repository
2. Create feature branch: `git checkout -b feature/YourFeature`
3. Maintain code documentation standards
4. Add unit tests for new functionality
5. Submit pull request with detailed description

## License

MIT License - see LICENSE file for details

## Citation

If you use this software in your research, please cite:

@software{inbora_gp_2025,
author = {Dr Chamyoung},
title = {Neural-Guided Genetic Programming for Symbolic Regression},
year = {2025},
publisher = {GitHub},
url = {https://github.com/InboraStudio/Neural-Guided-Genetic-Programming-for-Symbolic-Regression}
}
